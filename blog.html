<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog - Jason Zhang</title>
    <link rel="stylesheet" href="main.css">
</head>
<body>
    <nav>
        <div class="nav-left">
            <a href="index.html">Home</a>
        </div>
        
        <div class="nav-center">
            Jason Zhang
        </div>
        
        <div class="nav-right">
            <a href="https://x.com/minisounds" aria-label="Twitter">
                <img src="media/twitter.png" alt="Twitter Logo" class="nav-icon">
            </a>
            <a href="https://scholar.google.com/citations?user=LY1rce8AAAAJ&hl=en&authuser=1" aria-label="GitHub">
                <img src="media/googlescholar.svg" alt="GitHub Logo" class="nav-icon">
            </a>
            <a href="https://www.linkedin.com/in/jason-zhang-6860361b8/" aria-label="LinkedIn">
                <img src="media/linkedin.png" alt="LinkedIn Logo" class="nav-icon">
            </a>
        </div>
    </nav>

    <main class="container">
        <div class="blog-posts">
            <h1>Blog Posts</h1>

            <div class="blog-post-card">
                <h3>
                    <a href="blog_posts/benchmarks.html">Building Better Benchmarks: We Need Standardized AI Evaluation</a>
                </h3>
                <p class="post-date">December 11, 2024</p>
                <p>
                    AI benchmarking is in a state of disarray. From data leakage to reproducibility issues, our current evaluation methods raise serious questions about how we measure AI capabilities. This post explores the limitations of today's benchmarks and proposes a unified set of best practices for moving forward.
                </p>
            </div>
        </div>
    </main>
</body>
</html>